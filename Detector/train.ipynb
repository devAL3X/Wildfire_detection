{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "564b5672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "# helper functions from https://github.com/pytorch/vision/tree/main/references/detection\n",
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "import transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22c70c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"data/train/_annotations.csv\", header=None, names=[\"img\", \"xmin\", \"ymin\",        \n",
    "#                                                                     \"xmax\", \"ymax\", \"alpha\"])\n",
    "# df.drop([\"alpha\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91218932",
   "metadata": {},
   "source": [
    "Directory structure:\n",
    "```text\n",
    "data/\n",
    "├── test/\n",
    "│   └── _annotations.csv\n",
    "│   └── img1.jpg\n",
    "│   └── img2.jpg\n",
    "│   └── img3.jpg\n",
    "│   └── ...\n",
    "├── train/\n",
    "│   └── _annotations.csv\n",
    "│   └── img1.jpg\n",
    "│   └── img2.jpg\n",
    "│   └── img3.jpg\n",
    "│   └── ...\n",
    "└── valid/\n",
    "    └── _annotations.csv\n",
    "    └── img1.jpg\n",
    "    └── img2.jpg\n",
    "    └── img3.jpg\n",
    "    └── ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a432117a",
   "metadata": {},
   "source": [
    "# Creating dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c741810",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WildfireDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df_sample, annotations=\"_annotations.csv\", transforms=None):\n",
    "            '''\n",
    "            df_sample : could be 'train', 'test' or 'valid'\n",
    "            '''\n",
    "            self.df_sample = df_sample\n",
    "            self.annotations = annotations\n",
    "            self.transforms = transforms\n",
    "            self.path = \"data/\" + df_sample + \"/\"\n",
    "            self.df = pd.read_csv(self.path + annotations, header=None, names=[\"img\", \"xmin\", \"ymin\",        \n",
    "                                                                    \"xmax\", \"ymax\", \"alpha\"])\n",
    "            self.df.drop([\"alpha\"], axis=1, inplace=True)\n",
    "            self.img_names = self.df[\"img\"].to_numpy()\n",
    "            \n",
    "    def __getitem__(self, idx):\n",
    "        # load images and bounding boxes\n",
    "        img = Image.open(self.path + self.img_names[idx])\n",
    "        boxes = self.df[self.df[\"img\"] == self.img_names[idx]][self.df.columns[1:]].values\n",
    "        num_objs = len(boxes)\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        \n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64) # only one lable\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        target = {} # an array of values that will predict\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        \n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143b3bba",
   "metadata": {},
   "source": [
    "Test dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "621f1dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x480 at 0x7F6846E49F70>, {'boxes': tensor([[482., 296., 525., 320.]]), 'labels': tensor([1]), 'image_id': tensor([0]), 'area': tensor([1032.]), 'iscrowd': tensor([0])})\n"
     ]
    }
   ],
   "source": [
    "dataset = WildfireDataset(\"train\")\n",
    "print(dataset.__getitem__(0))\n",
    "%reset_selective -f dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3a0e67",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d751bdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c1a9155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    # converts the image, a PIL image, into a PyTorch Tensor\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        # during training, randomly flip the training images\n",
    "        # and ground-truth for data augmentation\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f846921d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Необработанный формат ячейки",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
